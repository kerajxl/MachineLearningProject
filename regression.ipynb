{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare necesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd \n",
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from datetime import datetime as dt \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading data and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic = pd.read_csv('traffic_train.csv')\n",
    "\n",
    "traffic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dayFromDate(traffic):\n",
    "    traffic['date_time'] = pd.to_datetime(traffic['date_time'])\n",
    "    traffic['hour'] = traffic.date_time.dt.hour\n",
    "    traffic['hour_interval'] = pd.cut(traffic.hour, bins = [-1,6,12,18,24], labels = ['0-6','6-12', '12-18', '18-24'])\n",
    "    \n",
    "    \n",
    "    traffic['day'] = traffic.date_time.dt.day_of_week  \n",
    "    \n",
    "    traffic['month'] = traffic.date_time.dt.month\n",
    "\n",
    "    traffic['tmp'] = range(0,len(traffic.hour))\n",
    "    traffic['year'] = traffic.date_time.dt.year\n",
    "    t1 = pd.get_dummies(traffic['month'])\n",
    "    t1['tmp'] = range(0,len(traffic.month))\n",
    "    traffic = pd.merge(traffic, t1, on = ['tmp'])\n",
    "\n",
    "    t1 = pd.get_dummies(traffic['hour_interval'])\n",
    "    t1['tmp'] = range(0,len(traffic.hour))\n",
    "    traffic = pd.merge(traffic, t1, on = ['tmp'])\n",
    "\n",
    "    t1 = pd.get_dummies(traffic['hour'])\n",
    "    t1['tmp'] = range(0,len(traffic.hour))\n",
    "    traffic = pd.merge(traffic, t1, on = ['tmp'])\n",
    "\n",
    "    t1 = pd.get_dummies(traffic['day'])\n",
    "    t1['tmp'] = range(0,len(traffic.hour))\n",
    "    traffic = pd.merge(traffic, t1, on = ['tmp']).drop(columns = ['tmp'])\n",
    "    return(traffic)\n",
    "\n",
    "def ordinal_whether(traffic):\n",
    "    traffic['ordinal_weather'] = np.where(traffic.weather_detailed.isin(['fog', 'freezing rain', 'heavy intensity rain','heavy snow','proximity thunderstorm',\n",
    "                                 'proximity thunderstorm with drizzle', 'proximity thunderstorm with rain', 'sleet', 'squalls',\n",
    "                                 'thunderstorm', 'thunderstorm with drizzle', 'thunderstorm with heavy rain', 'thunderstorm with light drizzle',\n",
    "                                  'thunderstorm with light rain', 'thunderstorm with rain', 'very heavy rain']), 0, #fatalne\n",
    "                                 np.where(traffic.weather_detailed.isin(['drizzle','haze', 'heavy intensity drizzle','light intensity drizzle','light intensity shower rain',\n",
    "                                 'light rain', 'light rain and snow', 'light shower snow', 'light snow','mist', 'moderate rain', 'shower drizzle',\n",
    "                                  'shower snow', 'smoke', 'snow', ]), 1, #kiepskie\n",
    "                                 np.where(traffic.weather_detailed.isin(['broken clouds','few clouds', 'overcast clouds', 'proximity shower rain',\n",
    "                                 ]), 2, #umiarkowane\n",
    "                                 np.where(traffic.weather_detailed.isin(['scattered clouds', 'sky is clear', ]), 3, 99 )))) #dobre\n",
    "    return (traffic)\n",
    "    \n",
    "def weather_dummies(traffic):\n",
    "    t1 = pd.get_dummies(traffic['weather_general'])\n",
    "    t1['tmp'] = range(0,len(traffic.weather_detailed))\n",
    "    traffic['tmp'] = range(0,len(traffic.weather_detailed))\n",
    "    traffic = pd.merge(traffic, t1, on = ['tmp'])\n",
    "    traffic['Fog'] = traffic['Fog'] + traffic['Smoke']\n",
    "    traffic['Rain'] = traffic['Rain'] + traffic['Squall']\n",
    "    traffic = traffic.drop(columns = ['Smoke', 'Squall', 'tmp'])\n",
    "    return(traffic)\n",
    "\n",
    "\n",
    "def outliers_correction(traffic):\n",
    "\n",
    "    traffic.temperature.loc[(traffic.temperature < -30)] = traffic.groupby(by = ['month']).mean().temperature[1]\n",
    "    traffic.rain_mm.loc[traffic.rain_mm > 1000] = traffic.rain_mm.loc[traffic.rain_mm < 1000].max()\n",
    "    return (traffic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericalVar = ['clouds_coverage_pct', 'temperature', 'rain_mm', 'snow_mm', 'traffic']\n",
    "fig, axs = plt.subplots(ncols=1, nrows=5, figsize=(12, 10),\n",
    "                        constrained_layout=True)\n",
    "for idx, col in enumerate(numericalVar):\n",
    "    axs[idx].hist(traffic[f'{col}'])\n",
    "    axs[idx].set_title(f'Variable: {col}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericalVar = ['clouds_coverage_pct', 'temperature', 'rain_mm', 'snow_mm', 'traffic']\n",
    "fig, axs = plt.subplots(ncols=1, nrows=5, figsize=(12, 10),\n",
    "                        constrained_layout=True)\n",
    "for idx, col in enumerate(numericalVar):\n",
    "    axs[idx].hist(traffic[f'{col}'])\n",
    "    axs[idx].set_title(f'Variable: {col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set(rc = {'figure.figsize':(20,10)})\n",
    "ax = sns.violinplot(x = traffic.hour_interval, y = traffic.traffic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.violinplot(x = traffic.hour, y = traffic.traffic, scale = \"width\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.violinplot(x = traffic.day, y = traffic.traffic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.violinplot(x = traffic.month, y = traffic.traffic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Standarize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale = StandardScaler()\n",
    "\n",
    "# numericalData = traffic[numericalVar]\n",
    "\n",
    "# scaledData = scale.fit_transform(numericalData)\n",
    "# traffic[numericalVar] = pd.DataFrame(scaledData)\n",
    "\n",
    "# traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Corrplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = traffic.corr()\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "corr[mask] = np.nan\n",
    "(corr\n",
    " .style\n",
    " .background_gradient(cmap='coolwarm', axis=None, vmin=-1, vmax=1)\n",
    " .highlight_null(null_color='#f1f1f1')  # Color NaNs grey\n",
    " .set_precision(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic = pd.read_csv('traffic_train.csv')\n",
    "functions = [dayFromDate(traffic), ordinal_whether(traffic), weather_dummies(traffic), outliers_correction(traffic)]\n",
    "\n",
    "traffic = dayFromDate(traffic)\n",
    "traffic = ordinal_whether(traffic)\n",
    "traffic = weather_dummies(traffic)\n",
    "traffic = outliers_correction(traffic)\n",
    "\n",
    "traffic = traffic[traffic['traffic'] != 0]\n",
    "traffic = traffic[traffic['snow_mm'] == 0]\n",
    "traffic = traffic[traffic['rain_mm'] == 0]\n",
    "traffic = traffic.drop(columns = ['date_time', 'weather_general', 'weather_detailed', 'month', 'year', 'day', 'hour', 'rain_mm','snow_mm','hour_interval'])\n",
    "#, '1_x',  '0-6','1_y','0_y'\n",
    "\n",
    "\n",
    "x_train = traffic.drop(columns = ['traffic'])\n",
    "y_train = traffic['traffic']\n",
    "\n",
    "print(traffic.columns)\n",
    "traffic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, r2_score\n",
    "lin_reg = LinearRegression(normalize=True)\n",
    "lin_reg.fit(x_train, y_train)\n",
    "\n",
    "y_pred = lin_reg.predict(x_train)\n",
    "\n",
    "print(mean_absolute_percentage_error(y_train, y_pred))\n",
    "print(mean_absolute_error(y_train, y_pred))\n",
    "print(r2_score(y_train, y_pred))\n",
    "print(y_pred)\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "# tree_reg.fit(x_train, y_train)\n",
    "# y_pred = tree_reg.predict(x_train)\n",
    "# tree_mape = mean_absolute_percentage_error(y_train, y_pred)\n",
    "# tree_mae = mean_absolute_error(y_train, y_pred)\n",
    "# print(tree_mape, tree_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestRegressor().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "pip_forest = Pipeline([(\"scaler\", RobustScaler()),\n",
    "                        (\"classifier\",RandomForestRegressor(random_state=997))])\n",
    "\n",
    "param_grid = [\n",
    "    {'classifier__n_estimators': [3, 10, 15, 20, 30], 'classifier__max_features': [2, 4, 6, 8,12]},\n",
    "    {'classifier__bootstrap': [False], 'classifier__n_estimators': [3, 10, 15,20,25, 30], 'classifier__max_features': [2, 4, 6, 8,12]}\n",
    "  ]\n",
    "\n",
    "\n",
    "forest_reg = RandomForestRegressor(random_state=997)\n",
    "# przeprowadza proces uczenia na pięciu podzbiorach, czyli łącznie (12+6)*5=90 przebiegów \n",
    "grid_search = GridSearchCV(pip_forest, param_grid, cv=10,\n",
    "                           scoring='neg_mean_absolute_percentage_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "best_forest = grid_search.best_estimator_ #RandomForestRegressor(bootstrap=False, max_features=3, n_estimators=3, random_state=42)\n",
    "best_forest.fit(x_train, y_train)\n",
    "y_forest = best_forest.predict(x_train)\n",
    "\n",
    "print(mean_absolute_percentage_error(y_train, y_forest))\n",
    "print(mean_absolute_error(y_train, y_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_forest = grid_search.best_estimator_ #RandomForestRegressor(bootstrap=False, max_features=3, n_estimators=3, random_state=42)\n",
    "best_forest.fit(x_train, y_train)\n",
    "y_forest = best_forest.predict(x_train)\n",
    "\n",
    "print(mean_absolute_percentage_error(y_train, y_forest))\n",
    "print(mean_absolute_error(y_train, y_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "pip_elastic = Pipeline([(\"scaler\", RobustScaler()),\n",
    "                        (\"classifier\",ElasticNet(random_state=997))])\n",
    "\n",
    "grid = dict()\n",
    "grid['classifier__alpha'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\n",
    "grid['classifier__l1_ratio'] = np.arange(0, 1, 0.2)\n",
    "\n",
    "# przeprowadza proces uczenia na pięciu podzbiorach, czyli łącznie (12+6)*5=90 przebiegów \n",
    "# ratios = arange(0, 1, 0.01)\n",
    "# alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\n",
    "# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# ElasticNetCV(l1_ratio=ratios, alphas=alphas, cv=cv, n_jobs=-1)\n",
    "grid_search = GridSearchCV(pip_elastic, grid, cv=5,\n",
    "                           scoring='neg_mean_absolute_percentage_error',\n",
    "                           return_train_score=True)\n",
    "best_elastic = grid_search #RandomForestRegressor(bootstrap=False, max_features=3, n_estimators=3, random_state=42)\n",
    "best_elastic.fit(x_train, y_train)\n",
    "\n",
    "y_elastic = best_elastic.predict(x_train)\n",
    "\n",
    "print(mean_absolute_percentage_error(y_train, y_elastic))\n",
    "print(mean_absolute_error(y_train, y_elastic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "x = ElasticNet(alpha= 0, l1_ratio = 0.0).fit(x_train, y_train)\n",
    "y_elastic = x.predict(x_train)\n",
    "\n",
    "print(mean_absolute_percentage_error(y_train, y_elastic))\n",
    "print(mean_absolute_error(y_train, y_elastic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "regressor = SVR(kernel = 'sigmoid')\n",
    "regressor.fit(x_train, y_train)\n",
    "\n",
    "svn_pred = regressor.predict(x_train)\n",
    "\n",
    "\n",
    "print(mean_absolute_percentage_error(y_train, svn_pred))\n",
    "print(mean_absolute_error(y_train, svn_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\"n_neighbors\": range(1, 20)}\n",
    "gridsearch = GridSearchCV(KNeighborsRegressor(), parameters)\n",
    "gridsearch.fit(x_train, y_train)\n",
    "# knn_model = KNeighborsRegressor(n_neighbors=3)\n",
    "# knn_model.fit(x_train, y_train)\n",
    "knn_predicted = gridsearch.predict(x_train)\n",
    "\n",
    "print(mean_absolute_percentage_error(y_train, knn_predicted))\n",
    "print(mean_absolute_error(y_train, knn_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_predicted = KNeighborsRegressor(n_neighbors=60).fit(x_train,y_train).predict(x_train)\n",
    "\n",
    "print(mean_absolute_percentage_error(y_train, knn_predicted))\n",
    "print(mean_absolute_error(y_train, knn_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.Lasso(alpha=0.1)\n",
    "reg.fit(x_train, y_train)\n",
    "y_reg = reg.predict(x_train)\n",
    "\n",
    "\n",
    "print(mean_absolute_percentage_error(y_train, y_reg))\n",
    "print(mean_absolute_error(y_train, y_reg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import TweedieRegressor\n",
    "reg = TweedieRegressor(power=1, alpha=0.1, link='log')\n",
    "reg.fit(x_train, y_train)\n",
    "y_reg = reg.predict(x_train)\n",
    "\n",
    "\n",
    "print(mean_absolute_percentage_error(y_train, y_reg))\n",
    "print(mean_absolute_error(y_train, y_reg))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "params = {\n",
    "    \"n_estimators\": 500,\n",
    "    \"max_depth\": 4,\n",
    "    \"min_samples_split\": 5,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"loss\": \"squared_error\",\n",
    "}\n",
    "reg = GradientBoostingRegressor(**params)\n",
    "reg.fit(x_train, y_train)\n",
    "\n",
    "y_reg = reg.predict(x_train)\n",
    "\n",
    "\n",
    "print(mean_absolute_percentage_error(y_train, y_reg))\n",
    "print(mean_absolute_error(y_train, y_reg))\n",
    "feature_importance = reg.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "plt.yticks(pos, np.array(x_train.columns)[sorted_idx])\n",
    "plt.title(\"Feature Importance (MDI)\")\n",
    "\n",
    "result = permutation_importance(\n",
    "    reg, x_train, y_train, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(\n",
    "    result.importances[sorted_idx].T,\n",
    "    vert=False,\n",
    "    labels=np.array(x_train.columns)[sorted_idx],\n",
    ")\n",
    "plt.title(\"Permutation Importance (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, r2_score\n",
    "\n",
    "params = {\n",
    "    'task': 'train', \n",
    "    'boosting': 'gbdt',\n",
    "    'n_estimators': 900,\n",
    "    'learning_rate': 0.3,\n",
    "    'objective': 'regression',\n",
    "    'num_leaves': 4096,\n",
    "    'max_depth' : 12,\n",
    "    'metric': {'l2','l1'},\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "\n",
    "\n",
    "model = lgb.train(params,\n",
    "                 train_set=lgb_train)\n",
    "                 #valid_sets=y_train,\n",
    "                 #early_stopping_rounds=30)\n",
    "\n",
    "\n",
    "lgm_pred = model.predict(x_train)\n",
    "\n",
    "\n",
    "print(mean_absolute_percentage_error(y_train, lgm_pred))\n",
    "print(mean_absolute_error(y_train, lgm_pred))\n",
    "\n",
    "x_ax = range(len(y_train))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_ax, y_train, label=\"original\")\n",
    "plt.plot(x_ax, lgm_pred, label=\"predicted\")\n",
    "plt.title(\" train and predicted data\")\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('traffic')\n",
    "plt.legend(loc='best',fancybox=True, shadow=False)\n",
    "plt.grid(True)\n",
    "plt.show()  \n",
    "\n",
    "lgb.plot_importance(model, height=.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\leski\\OneDrive\\Pulpit\\DS\\ML\\Projekt\\MachineLearningProject\\regression.ipynb Cell 32'\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leski/OneDrive/Pulpit/DS/ML/Projekt/MachineLearningProject/regression.ipynb#ch0000044?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m mean_absolute_percentage_error, mean_absolute_error, r2_score\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leski/OneDrive/Pulpit/DS/ML/Projekt/MachineLearningProject/regression.ipynb#ch0000044?line=3'>4</a>\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leski/OneDrive/Pulpit/DS/ML/Projekt/MachineLearningProject/regression.ipynb#ch0000044?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtask\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/leski/OneDrive/Pulpit/DS/ML/Projekt/MachineLearningProject/regression.ipynb#ch0000044?line=5'>6</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mboosting\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mgbdt\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leski/OneDrive/Pulpit/DS/ML/Projekt/MachineLearningProject/regression.ipynb#ch0000044?line=12'>13</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mverbose\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leski/OneDrive/Pulpit/DS/ML/Projekt/MachineLearningProject/regression.ipynb#ch0000044?line=13'>14</a>\u001b[0m }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/leski/OneDrive/Pulpit/DS/ML/Projekt/MachineLearningProject/regression.ipynb#ch0000044?line=15'>16</a>\u001b[0m lgb_train \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mDataset(x_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leski/OneDrive/Pulpit/DS/ML/Projekt/MachineLearningProject/regression.ipynb#ch0000044?line=18'>19</a>\u001b[0m model \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39mtrain(params,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leski/OneDrive/Pulpit/DS/ML/Projekt/MachineLearningProject/regression.ipynb#ch0000044?line=19'>20</a>\u001b[0m                  train_set\u001b[39m=\u001b[39mlgb_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leski/OneDrive/Pulpit/DS/ML/Projekt/MachineLearningProject/regression.ipynb#ch0000044?line=20'>21</a>\u001b[0m                  \u001b[39m#valid_sets=y_train,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/leski/OneDrive/Pulpit/DS/ML/Projekt/MachineLearningProject/regression.ipynb#ch0000044?line=21'>22</a>\u001b[0m                  \u001b[39m#early_stopping_rounds=30)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, r2_score\n",
    "\n",
    "params = {\n",
    "    'task': 'train', \n",
    "    'boosting': 'gbdt',\n",
    "    'n_estimators': 900,\n",
    "    'learning_rate': 0.3,\n",
    "    'objective': 'regression',\n",
    "    'num_leaves': 4096,\n",
    "    'max_depth' : 12,\n",
    "    'metric': {'l2','l1'},\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "\n",
    "\n",
    "model = lgb.train(params,\n",
    "                 train_set=lgb_train)\n",
    "                 #valid_sets=y_train,\n",
    "                 #early_stopping_rounds=30)\n",
    "\n",
    "\n",
    "lgm_pred = model.predict(x_train)\n",
    "\n",
    "\n",
    "print(mean_absolute_percentage_error(y_train, lgm_pred))\n",
    "print(mean_absolute_error(y_train, lgm_pred))\n",
    "\n",
    "x_ax = range(len(y_train))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_ax, y_train, label=\"original\")\n",
    "plt.plot(x_ax, lgm_pred, label=\"predicted\")\n",
    "plt.title(\" train and predicted data\")\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('traffic')\n",
    "plt.legend(loc='best',fancybox=True, shadow=False)\n",
    "plt.grid(True)\n",
    "plt.show()  \n",
    "\n",
    "lgb.plot_importance(model, height=.5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12080277744b494a6e3c186da3e912450d69eab61bfdf7b06084b016002dc5c4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
